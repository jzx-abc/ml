{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "254300c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [ True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]]])\n",
      "tensor([[[ 0.0909, -2.3515, -0.5661,  0.0241],\n",
      "         [ 0.5446, -0.6315, -1.2620,  0.4159],\n",
      "         [ 1.6499,  0.7056, -0.8306, -0.2571],\n",
      "         [ 0.3915, -0.3187, -0.2924, -2.7262]],\n",
      "\n",
      "        [[ 0.3916, -1.0063,  0.7366,  0.0394],\n",
      "         [-0.1752,  0.7153, -0.7634,  0.7525],\n",
      "         [-0.5505,  0.6407, -0.3635,  0.9172],\n",
      "         [ 0.2768,  0.2097,  1.0610, -1.1848]]])\n",
      "tensor([[[ 9.0882e-02, -2.3515e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [ 5.4460e-01, -6.3148e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[ 3.9160e-01, -1.0063e+00,  7.3659e-01,  3.9416e-02],\n",
      "         [-1.7523e-01,  7.1534e-01, -7.6338e-01,  7.5247e-01],\n",
      "         [-5.5055e-01,  6.4071e-01, -3.6354e-01,  9.1723e-01],\n",
      "         [ 2.7682e-01,  2.0969e-01,  1.0610e+00, -1.1848e+00]]])\n",
      "tensor([[[0.9200, 0.0800, 0.0000, 0.0000],\n",
      "         [0.7642, 0.2358, 0.0000, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
      "\n",
      "        [[0.2974, 0.0735, 0.4199, 0.2091],\n",
      "         [0.1534, 0.3737, 0.0852, 0.3878],\n",
      "         [0.1017, 0.3346, 0.1226, 0.4412],\n",
      "         [0.2295, 0.2146, 0.5027, 0.0532]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 关于word embedding，以序列建模为例\n",
    "# 考虑source sentence和target sentence\n",
    "# 构建序列，序列的字符以其在词表中的索引的形式表示\n",
    "batch_size = 2\n",
    "\n",
    "# 单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "model_dim = 8\n",
    "\n",
    "# 序列的最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "max_postion_len = 5\n",
    "\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "# 单词索引构成源句子和目标句子，并且做了padding，默认值为0\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len-L)), 0) \\\n",
    "                    for L in src_len])\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len-L)), 0) \\\n",
    "                    for L in tgt_len])\n",
    "\n",
    "# 构造embedding\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim) # max_num_src_words+1是因为进行了padding\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "\n",
    "# 构造position embedding\n",
    "pos_mat = torch.arange(max_postion_len).reshape(-1, 1)\n",
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape(1, -1)/model_dim)\n",
    "pe_embedding_table = torch.zeros(max_postion_len, model_dim)\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat) # 偶数\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat) # 奇数\n",
    "# print(pe_embedding_table)\n",
    "\n",
    "pe_embedding = nn.Embedding(max_postion_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "\n",
    "# 应该传入的是源句子或目标句子中单词的位置信息，而不是源句子或目标句子中的单词索引\n",
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) for _ in src_len]).to(torch.int32)\n",
    "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) for _ in tgt_len]).to(torch.int32)\n",
    "\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "\n",
    "# 构造encoder的self-attention mask\n",
    "# mask的shape：[batch_size, max_src_len, max_src_len]，值为1或-1e9\n",
    "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len)-L)), 0) \\\n",
    "                                               for L in src_len]), 2)\n",
    "valid_encoder_pos_matric = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "invalid_encoder_pos_matric = 1 - valid_encoder_pos_matric\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matric.to(torch.bool)\n",
    "\n",
    "score = torch.randn(batch_size, max(src_len), max(src_len))\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "\n",
    "print(mask_encoder_self_attention)\n",
    "print(score)\n",
    "print(masked_score)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65929be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0391, 0.0880, 0.1694, 0.0239, 0.1872])\n",
      "tensor([ 3.9077,  8.7990, 16.9395,  2.3876, 18.7243])\n",
      "tensor([0.1875, 0.1969, 0.2136, 0.1847, 0.2174])\n",
      "tensor([3.1463e-07, 4.1889e-05, 1.4371e-01, 6.8807e-08, 8.5625e-01])\n"
     ]
    }
   ],
   "source": [
    "# softmax演示\n",
    "alpha1 = 0.1\n",
    "alpha2 = 10\n",
    "score = torch.randn(5)\n",
    "pro1 = F.softmax(score*alpha1, -1)\n",
    "pro2 = F.softmax(score*alpha2, -1)\n",
    "def softmax_func(score):\n",
    "    return F.softmax(score)\n",
    "jaco_mat1 = torch.autograd.functional.jacobian(softmax_func, score*alpha1)\n",
    "jaco_mat2 = torch.autograd.functional.jacobian(softmax_func, score*alpha2)\n",
    "\n",
    "\n",
    "print(jaco_mat1)\n",
    "print(jaco_mat2)\n",
    "print(score*alpha1)\n",
    "print(score*alpha2)\n",
    "print(pro1)\n",
    "print(pro2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d3f59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 2, 0, 0, 0],\n",
      "        [7, 6, 2, 6, 0]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3002, -1.0248,  1.4143, -0.8677,  0.9682, -1.7056,  0.6163,  0.1819],\n",
      "        [-1.0613,  0.2050,  0.5995,  0.6321, -0.4734, -0.4036, -0.6138, -0.9511],\n",
      "        [-1.3335,  1.1304,  0.1107, -0.5590,  0.7528, -0.8151,  1.0783,  0.5061],\n",
      "        [ 1.2633, -0.2112, -0.2402,  0.3951, -1.7609,  1.0837, -2.3911,  0.8222],\n",
      "        [ 0.3705, -0.8774,  2.6517,  1.4675, -1.9497,  0.2747,  0.2293,  1.1696],\n",
      "        [ 0.8290,  0.1008, -0.4555,  1.2346, -0.1740, -1.0024, -0.8419, -1.1522],\n",
      "        [-0.0954, -0.4630,  1.3113,  0.0877,  1.1034,  1.2267, -1.0285, -0.2257],\n",
      "        [ 1.4446,  0.8707,  1.6861,  0.1696, -0.8664, -1.1610,  0.4321, -0.0873],\n",
      "        [-0.5472,  0.3052,  0.9246, -0.9573, -0.9849,  1.2678, -0.0442,  0.9531]],\n",
      "       requires_grad=True)\n",
      "tensor([[[ 0.3705, -0.8774,  2.6517,  1.4675, -1.9497,  0.2747,  0.2293,\n",
      "           1.1696],\n",
      "         [-1.3335,  1.1304,  0.1107, -0.5590,  0.7528, -0.8151,  1.0783,\n",
      "           0.5061],\n",
      "         [ 0.3002, -1.0248,  1.4143, -0.8677,  0.9682, -1.7056,  0.6163,\n",
      "           0.1819],\n",
      "         [ 0.3002, -1.0248,  1.4143, -0.8677,  0.9682, -1.7056,  0.6163,\n",
      "           0.1819],\n",
      "         [ 0.3002, -1.0248,  1.4143, -0.8677,  0.9682, -1.7056,  0.6163,\n",
      "           0.1819]],\n",
      "\n",
      "        [[ 1.4446,  0.8707,  1.6861,  0.1696, -0.8664, -1.1610,  0.4321,\n",
      "          -0.0873],\n",
      "         [-0.0954, -0.4630,  1.3113,  0.0877,  1.1034,  1.2267, -1.0285,\n",
      "          -0.2257],\n",
      "         [-1.3335,  1.1304,  0.1107, -0.5590,  0.7528, -0.8151,  1.0783,\n",
      "           0.5061],\n",
      "         [-0.0954, -0.4630,  1.3113,  0.0877,  1.1034,  1.2267, -1.0285,\n",
      "          -0.2257],\n",
      "         [ 0.3002, -1.0248,  1.4143, -0.8677,  0.9682, -1.7056,  0.6163,\n",
      "           0.1819]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(src_seq)\n",
    "print(src_embedding_table.weight)\n",
    "print(src_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd1445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb271e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260bf7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f029e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b96552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2047cf5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40050dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc82ff71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
